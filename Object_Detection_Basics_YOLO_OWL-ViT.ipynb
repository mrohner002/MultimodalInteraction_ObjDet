{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  - YOLO & OWL-ViT\n",
    "This tutorial demonstrates how to use YOLO (You Only Look Once) from the [Ultralytics](https://github.com/ultralytics/yolov5) library for object detection. It includes steps for:\n",
    "\n",
    "- Running object detection inference on images/videos\n",
    "- Fine-tuning YOLO for custom datasets\n",
    "- Comparing YOLO with OWl-VIT for zero-shot learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Object Detection Inference\n",
    "First thing We'll use YOLOv8 from Ultralyics for object detection on a sample image.\n",
    "We aim to utilize the pre-trained YOLOv8 model to detect objects in a sample image. This involves loading the model, providing an image for input, and interpreting the model's predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Inference**: The process of using a trained model to make predictions on new data.\n",
    "- **YOLOv8**: A state-of-the-art version of the YOLO (You Only Look Once) architecture, known for its speed and accuracy in object detection tasks.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the YOLOv8 model using the Ultralytics library.\n",
    "2. Perform inference on a sample image to detect objects.\n",
    "3. Visualize the results, including bounding boxes and class labels.\n",
    "\n",
    "**Support Material:**\n",
    "- https://docs.ultralytics.com/models/yolov8/\n",
    "- https://docs.ultralytics.com/tasks/detect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/street_scene.jpg: 384x640 13 persons, 1 bicycle, 9 cars, 2 motorcycles, 1 traffic light, 1 bench, 4 birds, 1 handbag, 1 potted plant, 174.6ms\n",
      "Speed: 6.2ms preprocess, 174.6ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 2.,  0.,  0.,  0., 58.,  0.,  2.,  9.,  0., 14.,  0.,  3.,  0.,  1.,  2., 14., 14.,  2.,  0.,  2.,  2.,  0.,  0., 26.,  0.,  3.,  2.,  0.,  0.,  2., 14., 13.,  2.])\n",
      "conf: tensor([0.9098, 0.9041, 0.9005, 0.8934, 0.8477, 0.8331, 0.8173, 0.7737, 0.7585, 0.7313, 0.6779, 0.6606, 0.6198, 0.5686, 0.5105, 0.5057, 0.5043, 0.4675, 0.4564, 0.4517, 0.4201, 0.4165, 0.4037, 0.4015, 0.3767, 0.3745, 0.3659, 0.3221, 0.3095, 0.3049, 0.2999, 0.2989, 0.2811])\n",
      "data: tensor([[9.5592e-01, 3.6429e+02, 6.0592e+02, 6.1893e+02, 9.0984e-01, 2.0000e+00],\n",
      "        [1.1789e+03, 4.2397e+02, 1.4806e+03, 8.6406e+02, 9.0414e-01, 0.0000e+00],\n",
      "        [1.5657e+03, 2.9039e+02, 1.7113e+03, 6.7585e+02, 9.0047e-01, 0.0000e+00],\n",
      "        [1.0586e+03, 4.0629e+02, 1.3231e+03, 7.3748e+02, 8.9344e-01, 0.0000e+00],\n",
      "        [1.9016e+02, 6.9791e+02, 4.1024e+02, 9.5744e+02, 8.4768e-01, 5.8000e+01],\n",
      "        [4.1516e+02, 5.8301e+02, 6.5140e+02, 9.2680e+02, 8.3306e-01, 0.0000e+00],\n",
      "        [3.7321e+02, 3.2695e+02, 6.1556e+02, 4.2069e+02, 8.1727e-01, 2.0000e+00],\n",
      "        [1.1758e+03, 3.8894e+01, 1.2183e+03, 1.5122e+02, 7.7372e-01, 9.0000e+00],\n",
      "        [9.3326e+02, 3.1249e+02, 1.0399e+03, 5.4408e+02, 7.5851e-01, 0.0000e+00],\n",
      "        [1.1763e+03, 9.2910e+02, 1.2933e+03, 1.0222e+03, 7.3131e-01, 1.4000e+01],\n",
      "        [5.9237e+02, 7.1440e+02, 1.1320e+03, 9.9478e+02, 6.7793e-01, 0.0000e+00],\n",
      "        [1.0581e+03, 3.7448e+02, 1.1869e+03, 5.0788e+02, 6.6059e-01, 3.0000e+00],\n",
      "        [7.4075e+02, 3.0132e+02, 8.6129e+02, 5.2405e+02, 6.1981e-01, 0.0000e+00],\n",
      "        [7.0015e+02, 4.1289e+02, 8.8419e+02, 5.7393e+02, 5.6856e-01, 1.0000e+00],\n",
      "        [5.7998e+02, 3.4540e+02, 7.9575e+02, 4.6612e+02, 5.1050e-01, 2.0000e+00],\n",
      "        [1.1354e+03, 8.4833e+02, 1.2484e+03, 9.4193e+02, 5.0570e-01, 1.4000e+01],\n",
      "        [9.5483e+02, 6.2117e+02, 9.9584e+02, 6.7102e+02, 5.0429e-01, 1.4000e+01],\n",
      "        [1.6384e+03, 3.6660e+02, 1.7917e+03, 4.9285e+02, 4.6751e-01, 2.0000e+00],\n",
      "        [1.3502e+03, 3.4881e+02, 1.3859e+03, 4.3120e+02, 4.5638e-01, 0.0000e+00],\n",
      "        [5.7960e+02, 3.4165e+02, 8.8617e+02, 4.6188e+02, 4.5173e-01, 2.0000e+00],\n",
      "        [1.6945e+03, 3.6793e+02, 1.7915e+03, 4.9295e+02, 4.2015e-01, 2.0000e+00],\n",
      "        [1.4419e+03, 3.4569e+02, 1.4719e+03, 4.4234e+02, 4.1646e-01, 0.0000e+00],\n",
      "        [1.0824e+03, 3.2515e+02, 1.1799e+03, 4.8252e+02, 4.0370e-01, 0.0000e+00],\n",
      "        [9.3231e+02, 3.7889e+02, 9.9831e+02, 4.4572e+02, 4.0154e-01, 2.6000e+01],\n",
      "        [1.1151e+03, 3.2569e+02, 1.1802e+03, 4.7567e+02, 3.7673e-01, 0.0000e+00],\n",
      "        [7.0181e+02, 4.0279e+02, 8.8495e+02, 5.7466e+02, 3.7447e-01, 3.0000e+00],\n",
      "        [1.5457e+03, 3.5707e+02, 1.7918e+03, 4.9037e+02, 3.6586e-01, 2.0000e+00],\n",
      "        [1.3430e+03, 3.4528e+02, 1.3785e+03, 4.3064e+02, 3.2208e-01, 0.0000e+00],\n",
      "        [1.4327e+03, 3.5082e+02, 1.4673e+03, 4.4290e+02, 3.0946e-01, 0.0000e+00],\n",
      "        [1.5333e+03, 3.7740e+02, 1.6210e+03, 4.8554e+02, 3.0489e-01, 2.0000e+00],\n",
      "        [1.0274e+03, 6.0857e+02, 1.0570e+03, 6.5136e+02, 2.9992e-01, 1.4000e+01],\n",
      "        [1.2430e+03, 6.1112e+02, 1.7259e+03, 1.0190e+03, 2.9888e-01, 1.3000e+01],\n",
      "        [8.6543e+02, 3.5008e+02, 9.4009e+02, 3.8912e+02, 2.8114e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1024, 1792)\n",
      "shape: torch.Size([33, 6])\n",
      "xywh: tensor([[ 303.4360,  491.6105,  604.9601,  254.6311],\n",
      "        [1329.7615,  644.0153,  301.6866,  440.0984],\n",
      "        [1638.5154,  483.1179,  145.6616,  385.4641],\n",
      "        [1190.8245,  571.8881,  264.4564,  331.1922],\n",
      "        [ 300.1982,  827.6760,  220.0836,  259.5260],\n",
      "        [ 533.2831,  754.9047,  236.2436,  343.7872],\n",
      "        [ 494.3851,  373.8213,  242.3596,   93.7392],\n",
      "        [1197.0276,   95.0568,   42.4672,  112.3248],\n",
      "        [ 986.5753,  428.2856,  106.6277,  231.5882],\n",
      "        [1234.8212,  975.6396,  117.0117,   93.0833],\n",
      "        [ 862.1802,  854.5883,  539.6152,  280.3784],\n",
      "        [1122.5028,  441.1805,  128.7488,  133.3917],\n",
      "        [ 801.0178,  412.6851,  120.5370,  222.7268],\n",
      "        [ 792.1692,  493.4145,  184.0449,  161.0408],\n",
      "        [ 687.8665,  405.7602,  215.7759,  120.7179],\n",
      "        [1191.9240,  895.1294,  112.9792,   93.6048],\n",
      "        [ 975.3347,  646.0970,   41.0160,   49.8470],\n",
      "        [1715.0410,  429.7282,  153.3424,  126.2471],\n",
      "        [1368.0439,  390.0021,   35.7831,   82.3866],\n",
      "        [ 732.8855,  401.7635,  306.5644,  120.2286],\n",
      "        [1743.0010,  430.4392,   97.0337,  125.0184],\n",
      "        [1456.9026,  394.0114,   30.0504,   96.6480],\n",
      "        [1131.1941,  403.8324,   97.5084,  157.3739],\n",
      "        [ 965.3071,  412.3028,   65.9980,   66.8322],\n",
      "        [1147.6650,  400.6788,   65.0867,  149.9766],\n",
      "        [ 793.3781,  488.7257,  183.1340,  171.8770],\n",
      "        [1668.7405,  423.7186,  246.1128,  133.3057],\n",
      "        [1360.7776,  387.9605,   35.5201,   85.3629],\n",
      "        [1449.9895,  396.8608,   34.6515,   92.0872],\n",
      "        [1577.1621,  431.4725,   87.7750,  108.1397],\n",
      "        [1042.2415,  629.9613,   29.6053,   42.7921],\n",
      "        [1484.4592,  815.0737,  482.9518,  407.9140],\n",
      "        [ 902.7610,  369.6018,   74.6650,   39.0404]])\n",
      "xywhn: tensor([[0.1693, 0.4801, 0.3376, 0.2487],\n",
      "        [0.7421, 0.6289, 0.1684, 0.4298],\n",
      "        [0.9144, 0.4718, 0.0813, 0.3764],\n",
      "        [0.6645, 0.5585, 0.1476, 0.3234],\n",
      "        [0.1675, 0.8083, 0.1228, 0.2534],\n",
      "        [0.2976, 0.7372, 0.1318, 0.3357],\n",
      "        [0.2759, 0.3651, 0.1352, 0.0915],\n",
      "        [0.6680, 0.0928, 0.0237, 0.1097],\n",
      "        [0.5505, 0.4182, 0.0595, 0.2262],\n",
      "        [0.6891, 0.9528, 0.0653, 0.0909],\n",
      "        [0.4811, 0.8346, 0.3011, 0.2738],\n",
      "        [0.6264, 0.4308, 0.0718, 0.1303],\n",
      "        [0.4470, 0.4030, 0.0673, 0.2175],\n",
      "        [0.4421, 0.4819, 0.1027, 0.1573],\n",
      "        [0.3839, 0.3963, 0.1204, 0.1179],\n",
      "        [0.6651, 0.8741, 0.0630, 0.0914],\n",
      "        [0.5443, 0.6310, 0.0229, 0.0487],\n",
      "        [0.9571, 0.4197, 0.0856, 0.1233],\n",
      "        [0.7634, 0.3809, 0.0200, 0.0805],\n",
      "        [0.4090, 0.3923, 0.1711, 0.1174],\n",
      "        [0.9727, 0.4204, 0.0541, 0.1221],\n",
      "        [0.8130, 0.3848, 0.0168, 0.0944],\n",
      "        [0.6312, 0.3944, 0.0544, 0.1537],\n",
      "        [0.5387, 0.4026, 0.0368, 0.0653],\n",
      "        [0.6404, 0.3913, 0.0363, 0.1465],\n",
      "        [0.4427, 0.4773, 0.1022, 0.1678],\n",
      "        [0.9312, 0.4138, 0.1373, 0.1302],\n",
      "        [0.7594, 0.3789, 0.0198, 0.0834],\n",
      "        [0.8091, 0.3876, 0.0193, 0.0899],\n",
      "        [0.8801, 0.4214, 0.0490, 0.1056],\n",
      "        [0.5816, 0.6152, 0.0165, 0.0418],\n",
      "        [0.8284, 0.7960, 0.2695, 0.3984],\n",
      "        [0.5038, 0.3609, 0.0417, 0.0381]])\n",
      "xyxy: tensor([[9.5592e-01, 3.6429e+02, 6.0592e+02, 6.1893e+02],\n",
      "        [1.1789e+03, 4.2397e+02, 1.4806e+03, 8.6406e+02],\n",
      "        [1.5657e+03, 2.9039e+02, 1.7113e+03, 6.7585e+02],\n",
      "        [1.0586e+03, 4.0629e+02, 1.3231e+03, 7.3748e+02],\n",
      "        [1.9016e+02, 6.9791e+02, 4.1024e+02, 9.5744e+02],\n",
      "        [4.1516e+02, 5.8301e+02, 6.5140e+02, 9.2680e+02],\n",
      "        [3.7321e+02, 3.2695e+02, 6.1556e+02, 4.2069e+02],\n",
      "        [1.1758e+03, 3.8894e+01, 1.2183e+03, 1.5122e+02],\n",
      "        [9.3326e+02, 3.1249e+02, 1.0399e+03, 5.4408e+02],\n",
      "        [1.1763e+03, 9.2910e+02, 1.2933e+03, 1.0222e+03],\n",
      "        [5.9237e+02, 7.1440e+02, 1.1320e+03, 9.9478e+02],\n",
      "        [1.0581e+03, 3.7448e+02, 1.1869e+03, 5.0788e+02],\n",
      "        [7.4075e+02, 3.0132e+02, 8.6129e+02, 5.2405e+02],\n",
      "        [7.0015e+02, 4.1289e+02, 8.8419e+02, 5.7393e+02],\n",
      "        [5.7998e+02, 3.4540e+02, 7.9575e+02, 4.6612e+02],\n",
      "        [1.1354e+03, 8.4833e+02, 1.2484e+03, 9.4193e+02],\n",
      "        [9.5483e+02, 6.2117e+02, 9.9584e+02, 6.7102e+02],\n",
      "        [1.6384e+03, 3.6660e+02, 1.7917e+03, 4.9285e+02],\n",
      "        [1.3502e+03, 3.4881e+02, 1.3859e+03, 4.3120e+02],\n",
      "        [5.7960e+02, 3.4165e+02, 8.8617e+02, 4.6188e+02],\n",
      "        [1.6945e+03, 3.6793e+02, 1.7915e+03, 4.9295e+02],\n",
      "        [1.4419e+03, 3.4569e+02, 1.4719e+03, 4.4234e+02],\n",
      "        [1.0824e+03, 3.2515e+02, 1.1799e+03, 4.8252e+02],\n",
      "        [9.3231e+02, 3.7889e+02, 9.9831e+02, 4.4572e+02],\n",
      "        [1.1151e+03, 3.2569e+02, 1.1802e+03, 4.7567e+02],\n",
      "        [7.0181e+02, 4.0279e+02, 8.8495e+02, 5.7466e+02],\n",
      "        [1.5457e+03, 3.5707e+02, 1.7918e+03, 4.9037e+02],\n",
      "        [1.3430e+03, 3.4528e+02, 1.3785e+03, 4.3064e+02],\n",
      "        [1.4327e+03, 3.5082e+02, 1.4673e+03, 4.4290e+02],\n",
      "        [1.5333e+03, 3.7740e+02, 1.6210e+03, 4.8554e+02],\n",
      "        [1.0274e+03, 6.0857e+02, 1.0570e+03, 6.5136e+02],\n",
      "        [1.2430e+03, 6.1112e+02, 1.7259e+03, 1.0190e+03],\n",
      "        [8.6543e+02, 3.5008e+02, 9.4009e+02, 3.8912e+02]])\n",
      "xyxyn: tensor([[5.3344e-04, 3.5576e-01, 3.3812e-01, 6.0442e-01],\n",
      "        [6.5788e-01, 4.1403e-01, 8.2623e-01, 8.4381e-01],\n",
      "        [8.7371e-01, 2.8358e-01, 9.5499e-01, 6.6001e-01],\n",
      "        [5.9073e-01, 3.9677e-01, 7.3831e-01, 7.2020e-01],\n",
      "        [1.0611e-01, 6.8156e-01, 2.2893e-01, 9.3500e-01],\n",
      "        [2.3167e-01, 5.6935e-01, 3.6351e-01, 9.0508e-01],\n",
      "        [2.0826e-01, 3.1929e-01, 3.4351e-01, 4.1083e-01],\n",
      "        [6.5614e-01, 3.7983e-02, 6.7983e-01, 1.4767e-01],\n",
      "        [5.2079e-01, 3.0517e-01, 5.8030e-01, 5.3133e-01],\n",
      "        [6.5643e-01, 9.0732e-01, 7.2172e-01, 9.9822e-01],\n",
      "        [3.3057e-01, 6.9766e-01, 6.3169e-01, 9.7146e-01],\n",
      "        [5.9047e-01, 3.6571e-01, 6.6232e-01, 4.9597e-01],\n",
      "        [4.1336e-01, 2.9426e-01, 4.8063e-01, 5.1177e-01],\n",
      "        [3.9071e-01, 4.0322e-01, 4.9341e-01, 5.6048e-01],\n",
      "        [3.2365e-01, 3.3731e-01, 4.4406e-01, 4.5519e-01],\n",
      "        [6.3361e-01, 8.2844e-01, 6.9666e-01, 9.1986e-01],\n",
      "        [5.3283e-01, 6.0661e-01, 5.5572e-01, 6.5529e-01],\n",
      "        [9.1427e-01, 3.5801e-01, 9.9984e-01, 4.8130e-01],\n",
      "        [7.5343e-01, 3.4063e-01, 7.7340e-01, 4.2109e-01],\n",
      "        [3.2344e-01, 3.3364e-01, 4.9451e-01, 4.5105e-01],\n",
      "        [9.4558e-01, 3.5931e-01, 9.9973e-01, 4.8139e-01],\n",
      "        [8.0462e-01, 3.3759e-01, 8.2139e-01, 4.3197e-01],\n",
      "        [6.0404e-01, 3.1752e-01, 6.5845e-01, 4.7121e-01],\n",
      "        [5.2026e-01, 3.7001e-01, 5.5709e-01, 4.3527e-01],\n",
      "        [6.2228e-01, 3.1806e-01, 6.5860e-01, 4.6452e-01],\n",
      "        [3.9164e-01, 3.9335e-01, 4.9383e-01, 5.6120e-01],\n",
      "        [8.6255e-01, 3.4870e-01, 9.9989e-01, 4.7888e-01],\n",
      "        [7.4945e-01, 3.3719e-01, 7.6927e-01, 4.2055e-01],\n",
      "        [7.9948e-01, 3.4259e-01, 8.1881e-01, 4.3252e-01],\n",
      "        [8.5562e-01, 3.6856e-01, 9.0460e-01, 4.7416e-01],\n",
      "        [5.7335e-01, 5.9430e-01, 5.8987e-01, 6.3609e-01],\n",
      "        [6.9363e-01, 5.9679e-01, 9.6313e-01, 9.9515e-01],\n",
      "        [4.8294e-01, 3.4188e-01, 5.2461e-01, 3.8000e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Run inference on a sample image\n",
    "\n",
    "results = model('/workspaces/MultimodalInteraction_ObjDet/images/street_scene.jpg', save = False)  # Displays image with detections\n",
    "\n",
    "for result in results:\n",
    "    print(result.boxes)  # Boxes object for bounding box outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning YOLO on Custom Dataset\n",
    "Fine-tuning YOLO requires a dataset formatted in the YOLO format. We'll use a small public dataset for demonstration.\n",
    "We will adapt the pre-trained YOLO model to a custom dataset. This process, known as fine-tuning, enables YOLO to specialize in detecting specific objects not included in its original training.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fine-tuning**: Adapting a pre-trained model to new data by continuing the training process.\n",
    "- **Custom Dataset**: A dataset that contains specific objects relevant to a new application, different from those YOLO was trained on (e.g. https://docs.ultralytics.com/datasets/detect/signature/.)\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare the custom dataset by organizing images and labels in the required format.\n",
    "2. Configure the YOLO training pipeline.\n",
    "3. Train the model and evaluate its performance.\n",
    "\n",
    "**Support Material:** \n",
    "- https://docs.ultralytics.com/modes/train/\n",
    "- https://docs.ultralytics.com/modes/val/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace ./datasets/signature.yaml? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# Download a sample dataset (e.g., Signature)\n",
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/signature.zip\n",
    "!unzip -q signature.zip -d ./datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.48 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.39 🚀 Python-3.10.15 torch-2.5.1+cu124 CPU (Intel Xeon Platinum 8370C 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./datasets/signature.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 58/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspaces/MultimodalInteraction_ObjDet/datasets/signature/train/labels.cache... 143 images, 0 backgrounds, 0 corrupt: 100%|██████████| 143/143 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspaces/MultimodalInteraction_ObjDet/datasets/signature/valid/labels.cache... 35 images, 0 backgrounds, 0 corrupt: 100%|██████████| 35/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      2.297      4.778      2.372         15        640: 100%|██████████| 9/9 [00:45<00:00,  5.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35    0.00114      0.343    0.00281   0.000597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.506      3.431       1.66         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35   0.000857      0.257     0.0105    0.00255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G     0.8896      2.422      1.173         15        640: 100%|██████████| 9/9 [00:43<00:00,  4.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35    0.00314      0.943      0.151       0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      0.751      1.916      1.021         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35    0.00525      0.914      0.509      0.174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G     0.5954      1.524     0.9224         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.951      0.857      0.916      0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G     0.5519      1.363     0.8872         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.996      0.886      0.935      0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.4727      1.224     0.8622         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.971      0.941      0.944      0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.4453      1.153     0.8461         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.998      0.943      0.945      0.836\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.4723      1.152     0.8562         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.998      0.943      0.974      0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.4202       1.12     0.8337         15        640: 100%|██████████| 9/9 [00:42<00:00,  4.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.997      0.943      0.982       0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.127 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.39 🚀 Python-3.10.15 torch-2.5.1+cu124 CPU (Intel Xeon Platinum 8370C 2.80GHz)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35      0.997      0.943      0.982       0.92\n",
      "Speed: 0.9ms preprocess, 55.0ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train YOLO on the dataset\n",
    "results = model.train(data='./datasets/signature.yaml', epochs=10, imgsz=640, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/example_signature.jpg: 640x480 (no detections), 80.5ms\n",
      "Speed: 2.5ms preprocess, 80.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # load a custom model, check the path depending on your output before!!\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(\"images/example_signature.jpg\", conf=0.75) #check params if you need to improve detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Learning with OWL-ViT\n",
    "Switch to `OWL-ViT` to see how it performs with zero-shot learning capabilities. Zero-shot means detecting objects without prior specific training.\n",
    "\n",
    "OWL-ViT (Open Vocabulary Learning with Vision Transformers) is a cutting-edge model designed for open vocabulary object detection. Unlike traditional models, OWL-ViT combines vision transformers with text embeddings, enabling it to:\\n\\n\n",
    "- Understand textual descriptions of objects, even if it hasn't seen them during training.\n",
    "- Detect and classify objects based on descriptive input, making it suitable for diverse applications.\n",
    "- Perform zero-shot learning by generalizing to new object classes without additional training.\\n\\n\"\n",
    "\n",
    "**Steps in Using OWL-ViT:**\n",
    "1. Model Initialization**: Set up the OWL-ViT model.\n",
    "2. Text Input for Object Descriptions: Provide descriptive prompts (e.g., 'a red car' or 'a black cat to guide detection.\n",
    "3. Inference and Visualization: Process an image or video, detect objects based on text descriptions and visualize results.\\n\\n\"\n",
    "\n",
    "OWL-ViT excels in scenarios where predefined object classes are insufficient, such as detecting rare or domain-specific objects.\n",
    "\n",
    "**Support Material**:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/owlvit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.333407998085022, 'label': 'a person on the floor', 'box': {'xmin': 584, 'ymin': 691, 'xmax': 1154, 'ymax': 1000}}, {'score': 0.1944514662027359, 'label': 'a person on the floor', 'box': {'xmin': 408, 'ymin': 595, 'xmax': 658, 'ymax': 923}}, {'score': 0.1605127900838852, 'label': 'a person on the floor', 'box': {'xmin': 1165, 'ymin': 425, 'xmax': 1484, 'ymax': 841}}, {'score': 0.15566213428974152, 'label': 'a person on the floor', 'box': {'xmin': 1040, 'ymin': 408, 'xmax': 1329, 'ymax': 718}}, {'score': 0.1443268209695816, 'label': 'a person on the floor', 'box': {'xmin': 1084, 'ymin': 326, 'xmax': 1179, 'ymax': 470}}, {'score': 0.1441059112548828, 'label': 'a person on the floor', 'box': {'xmin': 729, 'ymin': 301, 'xmax': 873, 'ymax': 509}}, {'score': 0.14044855535030365, 'label': 'a person on the floor', 'box': {'xmin': 1558, 'ymin': 294, 'xmax': 1722, 'ymax': 665}}, {'score': 0.13850808143615723, 'label': 'a person on the floor', 'box': {'xmin': 1337, 'ymin': 344, 'xmax': 1389, 'ymax': 425}}, {'score': 0.13026465475559235, 'label': 'a person on the floor', 'box': {'xmin': 1418, 'ymin': 349, 'xmax': 1474, 'ymax': 440}}, {'score': 0.12719741463661194, 'label': 'a person on the floor', 'box': {'xmin': 1264, 'ymin': 347, 'xmax': 1313, 'ymax': 428}}, {'score': 0.11922500282526016, 'label': 'a person on the floor', 'box': {'xmin': 1265, 'ymin': 347, 'xmax': 1309, 'ymax': 423}}, {'score': 0.11568083614110947, 'label': 'a church ', 'box': {'xmin': 706, 'ymin': 35, 'xmax': 870, 'ymax': 342}}, {'score': 0.1101481094956398, 'label': 'a person on the floor', 'box': {'xmin': 1295, 'ymin': 343, 'xmax': 1340, 'ymax': 421}}, {'score': 0.10863406956195831, 'label': 'a person on the floor', 'box': {'xmin': 932, 'ymin': 316, 'xmax': 1046, 'ymax': 524}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "image = Image.open(\"images/street_scene.jpg\")\n",
    "\n",
    "\n",
    "def preprocess_outputs(output):\n",
    "    input_scores = [x[\"score\"] for x in output]\n",
    "    input_labels = [x[\"label\"] for x in output]\n",
    "    input_boxes = []\n",
    "    for i in range(len(output)):\n",
    "        input_boxes.append([*output[i][\"box\"].values()])\n",
    "    input_boxes = [input_boxes]\n",
    "    return input_scores, input_labels, input_boxes\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for i, box in enumerate(boxes):\n",
    "        show_box(box, plt.gca())\n",
    "        plt.text(\n",
    "            x=box[0],\n",
    "            y=box[1] - 12,\n",
    "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
    "            c=\"beige\",\n",
    "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
    "        )\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "OWL_checkpoint = \"google/owlvit-base-patch32\"\n",
    "\n",
    "text = [\"a person on the floor\", \"a church \"]\n",
    "\n",
    "# Load the model\n",
    "detector = pipeline(\n",
    "    model= OWL_checkpoint,\n",
    "    task=\"zero-shot-object-detection\"\n",
    ")\n",
    "\n",
    "output = detector(\n",
    "    image,\n",
    "    candidate_labels = text\n",
    ")\n",
    "\n",
    "print(output)\n",
    "\n",
    "input_scores, input_labels, input_boxes = preprocess_outputs(output)\n",
    "\n",
    "# Show the image with the bounding boxes\n",
    "show_boxes_and_labels_on_image(\n",
    "    image,\n",
    "    input_boxes[0],\n",
    "    input_labels,\n",
    "    input_scores\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
